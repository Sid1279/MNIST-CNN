{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5ecc503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "fe068919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransform(object):\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.grad_weights = np.zeros_like(W)\n",
    "        self.grad_biases = np.zeros_like(b)\n",
    "        self.velocity_weights = np.zeros_like(W)\n",
    "        self.velocity_biases = np.zeros_like(b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "        #x -> vector, W -> weight matrix, b -> bias vector\n",
    "        #x -> input_dims\n",
    "        #W -> input_dims x output_dims\n",
    "        #b -> output_dims\n",
    "\n",
    "    def backward(self, grad_output, x, learning_rate=0.0, momentum=0.0, l2_penalty=0.0):\n",
    "        self.grad_weights = np.dot(x.T, grad_output)\n",
    "        #gradient wrt Weight matrix\n",
    "        #shape(x) = (batch_size, input_dims) => shape(x.T) = (input_dims, batch_size)\n",
    "        #shape (grad_output) = (batch_size, output_dims)\n",
    "        #x_T * grad_output = (input_dims, batch_size) * (batch_size, output_dims) = (input_dims, output_dims) = dim(W)\n",
    "         \n",
    "        self.grad_biases = np.sum(grad_output, axis=0)\n",
    "        #shape (grad_output) = (batch_size, output_dims)\n",
    "        #shape (grad_biases) = (output_dims,)\n",
    "        #sums the elements of grad_output across its rows, outputs an array.\n",
    "        #contains the gradients of the loss with respect to each element of the bias term b\n",
    "\n",
    "        grad_x = np.dot(grad_output, self.W.T)\n",
    "        #gradient wrt input vector x\n",
    "        #shape (grad_output) = (batch_size, output_dims)\n",
    "        #shape(W) = (input_dims, output_dims) => shape(W.T) = (output_dims, input_dims)\n",
    "        #grad_output * W.T = (batch_size, output_dims) * (output_dims, input_dims) = (batch_size, input_dims) = dim(x\n",
    "\n",
    "        # Update weights and biases using gradient descent with momentum and L2 regularization\n",
    "        self.velocity_weights = momentum * self.velocity_weights - learning_rate * (self.grad_weights + l2_penalty * self.W)        \n",
    "        self.velocity_biases = momentum * self.velocity_biases - learning_rate * self.grad_biases\n",
    "        #The velocity is updated by taking the previous velocity and \n",
    "        #subtracting the current gradient multiplied by the learning rate (learning_rate)\n",
    "\n",
    "        self.W += self.velocity_weights\n",
    "        self.b += self.velocity_biases\n",
    "        #After updating the velocities, the weights and biases are updated by adding the velocity terms\n",
    "        #The momentum term (momentum) controls how much of the previous velocity is retained and how much of the current gradient update is incorporated.\n",
    "\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ed8a93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x, 0)\n",
    "        #ReLU(x) = max(x,0)\n",
    "        #ReLU(x) = 0 if x < 0; x if x > 0\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        grad_output,\n",
    "        learning_rate=0.0,\n",
    "        momentum=0.0,\n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        return grad_output * (self.x > 0)\n",
    "        #if x<0, the gradient will be 0\n",
    "        #if x>0, the gradient will be grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "37b179a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidCrossEntropy(object):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        # Apply clipping to limit the range of input values; prevent overflow from occuring\n",
    "        #sig(x) = 1/(1 + e^-x)\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        grad_output,\n",
    "        learning_rate=0.0,\n",
    "        momentum=0.0,\n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        sigmoid_x = self.forward(self.x)\n",
    "        return sigmoid_x * (1 - sigmoid_x) * grad_output\n",
    "        #Derivative wrt x:\n",
    "        #sig'(x) = (sig(x))*(1-sig(x))\n",
    "        #check why its being multiplied by gradient of next layer: \n",
    "        #CHAIN RULE! grad_output is a constant being multiplied by the sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "33e1ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, input_dims, hidden_units):\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_units = hidden_units\n",
    "        self.W1 = np.random.randn(self.input_dims, self.hidden_units)\n",
    "        self.b1 = np.zeros(self.hidden_units)\n",
    "        self.W2 = np.random.randn(self.hidden_units, 1)\n",
    "        self.b2 = np.zeros(1)\n",
    "\n",
    "        #Initialize the first linear transform matrix with a random weight matrix & bias vector\n",
    "        self.linear_transform1 = LinearTransform(\n",
    "            np.random.randn(input_dims, hidden_units),\n",
    "            np.random.randn(hidden_units),\n",
    "        )\n",
    "        \n",
    "        #Initialize the second (output) linear transform matrix with a random weight matrix & bias vector\n",
    "        #Desired number of output units: 1\n",
    "        self.linear_transform2 = LinearTransform(\n",
    "            np.random.randn(hidden_units, 1),\n",
    "            np.random.randn(1),\n",
    "        )\n",
    "\n",
    "        # Initialize ReLU activation function\n",
    "        self.relu = ReLU()\n",
    "        \n",
    "        # Initialize sigmoid loss function\n",
    "        self.sigmoid_cross_entropy = SigmoidCrossEntropy()\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x_batch,\n",
    "        y_batch,\n",
    "        learning_rate,\n",
    "        momentum,\n",
    "        l2_penalty,\n",
    "    ):\n",
    "        # Forward pass\n",
    "        hidden_output = self.linear_transform1.forward(x_batch)\n",
    "        hidden_output_relu = self.relu.forward(hidden_output)\n",
    "        final_output = self.linear_transform2.forward(hidden_output_relu)\n",
    "        predicted_labels = self.sigmoid_cross_entropy.forward(final_output)\n",
    "\n",
    "        # Backward pass\n",
    "        grad_output = predicted_labels - y_batch\n",
    "        grad_hidden_output_relu = self.sigmoid_cross_entropy.backward(grad_output, final_output)\n",
    "        grad_hidden_output = self.relu.backward(grad_hidden_output_relu, hidden_output_relu)\n",
    "\n",
    "        # Update the second layer weights and biases\n",
    "        grad_weights2 = np.dot(hidden_output_relu.T, grad_output).reshape(self.linear_transform2.W.shape)\n",
    "        grad_biases2 = np.sum(grad_output, axis=0)\n",
    "        self.linear_transform2.backward(grad_output, hidden_output_relu, learning_rate, momentum, l2_penalty)\n",
    "\n",
    "        # Update the first layer weights and biases\n",
    "        grad_input = self.linear_transform1.backward(grad_hidden_output, x_batch, learning_rate, momentum, l2_penalty)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        # Forward pass\n",
    "        hidden_output = self.linear_transform1.forward(x)\n",
    "        hidden_output_relu = self.relu.forward(hidden_output)\n",
    "        final_output = self.linear_transform2.forward(hidden_output_relu)\n",
    "        predicted_labels = self.sigmoid_cross_entropy.forward(final_output)\n",
    "\n",
    "        # Compute accuracy\n",
    "        predicted_labels_binary = (predicted_labels >= 0.5).astype(int)\n",
    "        accuracy = np.mean(predicted_labels_binary == y)\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def compute_loss(self, x, y):\n",
    "        linear1 = LinearTransform(self.W1, self.b1)\n",
    "        relu = ReLU()\n",
    "        linear2 = LinearTransform(self.W2, self.b2)\n",
    "        sigmoid_ce = SigmoidCrossEntropy()\n",
    "\n",
    "        h1 = linear1.forward(x)\n",
    "        h1_relu = relu.forward(h1)\n",
    "        h2 = linear2.forward(h1_relu)\n",
    "        y_pred = sigmoid_ce.forward(h2)\n",
    "\n",
    "        loss = np.mean(y_pred)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e16134f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('cifar_2class_py2.p', 'rb'), encoding='bytes')\n",
    "\n",
    "train_x = data[b'train_data']\n",
    "train_y = data[b'train_labels']\n",
    "test_x = data[b'test_data']\n",
    "test_y = data[b'test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "dbd3a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples, input_dims = train_x.shape\n",
    "hidden_units = 1000\n",
    "\n",
    "num_epochs = 10\n",
    "num_batches = 100\n",
    "learning_rate = 0.092\n",
    "momentum = 0.9\n",
    "l2_penalty = 0.001\n",
    "\n",
    "mlp = MLP(input_dims, hidden_units)\n",
    "\n",
    "batch_size = num_examples // num_batches  # Calculate batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 100]    Avg.Loss = 0.740\n",
      "Train Loss: 0.754    Train Accuracy: 73.70%\n",
      "Test Loss:  0.767    Test Accuracy:  74.20%\n",
      "[Epoch 2, mb 100]    Avg.Loss = 0.740\n",
      "Train Loss: 0.754    Train Accuracy: 74.89%\n",
      "Test Loss:  0.767    Test Accuracy:  75.20%\n",
      "[Epoch 3, mb 100]    Avg.Loss = 0.740\n",
      "Train Loss: 0.754    Train Accuracy: 75.19%\n",
      "Test Loss:  0.767    Test Accuracy:  75.90%\n",
      "[Epoch 4, mb 100]    Avg.Loss = 0.740\n",
      "Train Loss: 0.754    Train Accuracy: 75.21%\n",
      "Test Loss:  0.767    Test Accuracy:  74.75%\n",
      "[Epoch 5, mb 100]    Avg.Loss = 0.740\n",
      "Train Loss: 0.754    Train Accuracy: 50.33%\n",
      "Test Loss:  0.767    Test Accuracy:  50.45%\n",
      "[Epoch 6, mb 100]    Avg.Loss = 0.740\n",
      "Train Loss: 0.754    Train Accuracy: 75.85%\n",
      "Test Loss:  0.767    Test Accuracy:  74.15%\n",
      "[Epoch 7, mb 3]    Avg.Loss = 0.740"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.random.permutation(num_examples)\n",
    "    train_x_shuffled = train_x[indices]\n",
    "    train_y_shuffled = train_y[indices]\n",
    "\n",
    "#     total_loss = 0.0\n",
    "    for batch in range(num_batches):\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Select the current batch of examples and labels\n",
    "        batch_x = train_x_shuffled[start_idx:end_idx]\n",
    "        batch_y = train_y_shuffled[start_idx:end_idx]\n",
    "\n",
    "        # Train the MLP on the current batch\n",
    "        mlp.train(batch_x, batch_y, learning_rate, momentum, l2_penalty)\n",
    "        loss = mlp.compute_loss(batch_x, batch_y)\n",
    "#         total_loss += loss\n",
    "#         total_loss /= num_batches\n",
    "\n",
    "        print(\n",
    "            '\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "            ),\n",
    "            end='',\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    test_loss = mlp.compute_loss(test_x, test_y)\n",
    "    train_loss =  mlp.compute_loss(train_x, train_y)\n",
    "    train_accuracy = mlp.evaluate(train_x, train_y)\n",
    "    test_accuracy = mlp.evaluate(test_x, test_y)\n",
    "\n",
    "    print()\n",
    "    print('Train Loss: {:.3f}    Train Accuracy: {:.2f}%'.format(\n",
    "        train_loss,\n",
    "        100. * train_accuracy,\n",
    "    ))\n",
    "    print('Test Loss:  {:.3f}    Test Accuracy:  {:.2f}%'.format(\n",
    "        test_loss,\n",
    "        100. * test_accuracy,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accuracies = []\n",
    "# test_accuracies = []\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Shuffle the training data\n",
    "#     indices = np.random.permutation(num_examples)\n",
    "#     train_x_shuffled = train_x[indices]\n",
    "#     train_y_shuffled = train_y[indices]\n",
    "\n",
    "#     total_loss = 0.0\n",
    "#     for batch in range(num_batches):\n",
    "#         start_idx = batch * batch_size\n",
    "#         end_idx = start_idx + batch_size\n",
    "\n",
    "#         # Select the current batch of examples and labels\n",
    "#         batch_x = train_x_shuffled[start_idx:end_idx]\n",
    "#         batch_y = train_y_shuffled[start_idx:end_idx]\n",
    "\n",
    "#         # Train the MLP on the current batch\n",
    "#         mlp.train(batch_x, batch_y, learning_rate, momentum, l2_penalty)\n",
    "#         loss = mlp.compute_loss(batch_x, batch_y)\n",
    "#         total_loss += loss\n",
    "\n",
    "#         print(\n",
    "#             '\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(\n",
    "#                 epoch + 1,\n",
    "#                 batch + 1,\n",
    "#                 loss,\n",
    "#             ),\n",
    "#             end='',\n",
    "#         )\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "#     total_loss /= num_batches\n",
    "\n",
    "#     train_accuracy = mlp.evaluate(train_x, train_y)\n",
    "#     test_accuracy = mlp.evaluate(test_x, test_y)\n",
    "    \n",
    "#     train_accuracies.append(train_accuracy)\n",
    "#     test_accuracies.append(test_accuracy)\n",
    "\n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs} - Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# test_loss = mlp.compute_loss(test_x, test_y)\n",
    "# train_loss = total_loss\n",
    "# overall_train_accuracy = sum(train_accuracies)/num_epochs\n",
    "# overall_test_accuracy = sum(test_accuracies)/num_epochs\n",
    "\n",
    "# print()\n",
    "# print('Train Loss: {:.3f}    Overall Train Accuracy: {:.2f}%'.format(\n",
    "#     train_loss,\n",
    "#     100. * overall_train_accuracy,\n",
    "# ))\n",
    "# print('Test Loss:  {:.3f}    Overall Test Accuracy:  {:.2f}%'.format(\n",
    "#     test_loss,\n",
    "#     100. * overall_test_accuracy,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     # Shuffle the training data\n",
    "#     indices = np.random.permutation(num_examples)\n",
    "#     train_x_shuffled = train_x[indices]\n",
    "#     train_y_shuffled = train_y[indices]\n",
    "\n",
    "#     total_loss = 0.0\n",
    "#     for batch in range(num_batches):\n",
    "#         start_idx = batch * batch_size\n",
    "#         end_idx = start_idx + batch_size\n",
    "\n",
    "#         # Select the current batch of examples and labels\n",
    "#         batch_x = train_x_shuffled[start_idx:end_idx]\n",
    "#         batch_y = train_y_shuffled[start_idx:end_idx]\n",
    "\n",
    "#         # Train the MLP on the current batch\n",
    "#         mlp.train(batch_x, batch_y, learning_rate, momentum, l2_penalty)\n",
    "#         loss = mlp.compute_loss(batch_x, batch_y)\n",
    "#         total_loss += loss\n",
    "\n",
    "#         print(\n",
    "#             '\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(\n",
    "#                 epoch + 1,\n",
    "#                 batch + 1,\n",
    "#                 loss,\n",
    "#             ),\n",
    "#             end='',\n",
    "#         )\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "#     total_loss /= num_batches\n",
    "#     train_loss = total_loss\n",
    "#     train_accuracy = mlp.evaluate(train_x, train_y)\n",
    "#     test_accuracy = mlp.evaluate(test_x, test_y)\n",
    "\n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs} - Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# test_loss = mlp.compute_loss(test_x, test_y)\n",
    "# train_loss = total_loss\n",
    "# test_accuracy = mlp.evaluate(test_x, test_y)\n",
    "# train_accuracy = mlp.evaluate(train_x, train_y)\n",
    "\n",
    "# print()\n",
    "# print('Train Loss: {:.3f}    Train Accuracy: {:.2f}%'.format(\n",
    "#     train_loss,\n",
    "#     100. * train_accuracy,\n",
    "# ))\n",
    "# print('Test Loss:  {:.3f}    Test Accuracy:  {:.2f}%'.format(\n",
    "#     test_loss,\n",
    "#     100. * test_accuracy,\n",
    "# ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
